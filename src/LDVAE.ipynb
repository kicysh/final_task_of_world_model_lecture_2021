{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDVAE.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPGZOXpuIqxcQMZFTGaV2vg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kicysh/final_task_of_world_model_lecture_2021/blob/main/src/LDVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip"
      ],
      "metadata": {
        "id": "mF-ceB4dTt1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2EsievTT474",
        "outputId": "1c60187c-27ba-4ce5-fb41-72cc999c3011"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.11.2)\n",
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.63.0)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.7 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.8.1)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sinfo\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=ce0d5a0f2bf0596473d668297e76f4af2268f270faafd5f4f1270e8c2a4596b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=56a5027cdecc4b860edcc725a10fbaf19c17c12e983aa72740328bcd75d2d078\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=a4cd6fa349dd3c1b0d316ea5defcf78954361d9c6af9397bd7eed16c1b32c3f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built umap-learn pynndescent sinfo\n",
            "Installing collected packages: stdlib-list, pynndescent, umap-learn, sinfo, anndata, scanpy\n",
            "Successfully installed anndata-0.8.0 pynndescent-0.5.6 scanpy-1.8.2 sinfo-0.3.4 stdlib-list-0.8.0 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "kZnO-gmJTTwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!gsutil cp gs://h5ad/2019-02-Pijuan-Sala-et-al-Nature/pijuan_sala_atlas.h5ad /content/data\n",
        "path_of_data = '/content/data/pijuan_sala_atlas.h5ad'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCBTOxD6Tg7h",
        "outputId": "4fffc7c0-00ed-4b03-85ce-0922844d4419"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://h5ad/2019-02-Pijuan-Sala-et-al-Nature/pijuan_sala_atlas.h5ad...\n",
            "| [1 files][  1.0 GiB/  1.0 GiB]   47.1 MiB/s                                   \n",
            "Operation completed over 1 objects/1.0 GiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setting\n"
      ],
      "metadata": {
        "id": "47p9z8QQs4bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting\n",
        "SETTING_BATCHNORM_EPS = 0.001\n",
        "SETTING_BATCHNORM_MOMENTUM = 0.01\n",
        "SETTING_ENCODER_Z_DROPOUT_P = 0.1\n",
        "SETTING_ENCODER_L_DROPOUT_P = 0.1\n",
        "SETTING_HIDDEN_DIM = 128\n",
        "SETTING_EPS = 1e-8\n",
        "\n",
        "USE_CUDA = True\n",
        "SETTING_BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "grS7L_2ms24J"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "Q0rzv6RVs9sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HFG8utJ7LYQm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import logsumexp\n",
        "from torch.distributions import Normal, kl_divergence\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(1234)\n",
        "random_state = 42\n",
        "\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "adata = sc.read_h5ad(path_of_data)\n",
        "adata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BzFALOqUJ-j",
        "outputId": "9e786e8d-051f-4f97-df4a-e4f59ae9c0be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 139331 × 29452\n",
              "    obs: 'barcode', 'sample', 'stage', 'sequencing.batch', 'theiler', 'doub.density', 'doublet', 'cluster', 'cluster.sub', 'cluster.stage', 'cluster.theiler', 'stripped', 'celltype', 'colour', 'umapX', 'umapY', 'haem_gephiX', 'haem_gephiY', 'haem_subclust', 'endo_gephiX', 'endo_gephiY', 'endo_trajectoryName', 'endo_trajectoryDPT', 'endo_gutX', 'endo_gutY', 'endo_gutDPT', 'endo_gutCluster'\n",
              "    var: 'gene_name'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = adata.obs.query('not doublet').index\n",
        "idx = np.random.choice(idx, 20000, replace=False)\n",
        "sc.pp.filter_genes(adata, min_cells=100)\n",
        "adata = adata[idx]"
      ],
      "metadata": {
        "id": "I81c3-hrQeL9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrsy2EGiSJU4",
        "outputId": "0c76edc8-5b6d-4277-c53f-62dbcd9eb535"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "View of AnnData object with n_obs × n_vars = 20000 × 17006\n",
              "    obs: 'barcode', 'sample', 'stage', 'sequencing.batch', 'theiler', 'doub.density', 'doublet', 'cluster', 'cluster.sub', 'cluster.stage', 'cluster.theiler', 'stripped', 'celltype', 'colour', 'umapX', 'umapY', 'haem_gephiX', 'haem_gephiY', 'haem_subclust', 'endo_gephiX', 'endo_gephiY', 'endo_trajectoryName', 'endo_trajectoryDPT', 'endo_gutX', 'endo_gutY', 'endo_gutDPT', 'endo_gutCluster'\n",
              "    var: 'gene_name', 'n_cells'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from math import ldexp\n",
        "\n",
        "class LDVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    :param genes_cnt: Number of input genes\n",
        "    :param latent_dim: Dimensionality of the latent space \n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        genes_cnt: int, \n",
        "        latent_dim: int = 20\n",
        "    ):\n",
        "        super(LDVAE,self).__init__()\n",
        "        self.local_l_mean = None\n",
        "        self.local_l_std = None\n",
        "        self.eps = SETTING_EPS\n",
        "\n",
        "        self.theta = nn.Parameter(torch.randn(genes_cnt))\n",
        "        self.encoder_z = nn.Sequential(\n",
        "            nn.Linear(genes_cnt, SETTING_HIDDEN_DIM),\n",
        "            nn.BatchNorm1d(SETTING_HIDDEN_DIM,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(SETTING_ENCODER_Z_DROPOUT_P)\n",
        "        )\n",
        "        self.encoder_z_mean = nn.Linear(SETTING_HIDDEN_DIM,latent_dim)\n",
        "        self.encoder_z_std = nn.Linear(SETTING_HIDDEN_DIM,latent_dim)\n",
        "\n",
        "        self.encoder_l = nn.Sequential(\n",
        "            nn.Linear(genes_cnt, SETTING_HIDDEN_DIM),\n",
        "            nn.BatchNorm1d(SETTING_HIDDEN_DIM,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(SETTING_ENCODER_L_DROPOUT_P)\n",
        "        )\n",
        "        self.encoder_l_mean = nn.Linear(SETTING_HIDDEN_DIM, 1)\n",
        "        self.encoder_l_std = nn.Linear(SETTING_HIDDEN_DIM, 1)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, genes_cnt),\n",
        "            nn.BatchNorm1d(genes_cnt,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_z = self.encoder_z(x)\n",
        "        z_mean = self.encoder_z_mean(x_z)\n",
        "        z_std = torch.exp(self.encoder_z_std(x_z)) \n",
        "        z = Normal(z_mean, z_std.sqrt()).rsample()\n",
        "\n",
        "        x_l = self.encoder_l(x)\n",
        "        l_mean = self.encoder_l_mean(x_l)\n",
        "        l_std = torch.exp(self.encoder_l_std(x_l))\n",
        "        library = Normal(l_mean, l_std.sqrt()).rsample()\n",
        "\n",
        "        y = self.decoder(z)\n",
        "        y = torch.exp(library)*torch.softmax(y, dim=-1)\n",
        "        return [z_mean, z_std, z], [l_mean, l_std, library], y\n",
        "\n",
        "\n",
        "    def set_local_l_mean_and_std(self, data):\n",
        "        masked_log_sum =np.ma.log(data.sum(axis=1))\n",
        "        log_counts = masked_log_sum.filled(0)\n",
        "        self.local_l_mean = (np.mean(log_counts).reshape(-1, 1)).astype(np.float32)[0][0]\n",
        "        self.local_l_std = (np.var(log_counts).reshape(-1, 1)).astype(np.float32)[0][0]\n",
        "        return self.local_l_mean, self.local_l_std\n",
        "\n",
        "\n",
        "    def reconst_error(self,x, mu, theta):\n",
        "        eps = SETTING_EPS\n",
        "        log_theta_mu_eps = torch.log(theta + mu + eps)\n",
        "\n",
        "        res = (\n",
        "            theta * (torch.log(theta + eps) - log_theta_mu_eps)\n",
        "            + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
        "            + torch.lgamma(x + theta)\n",
        "            - torch.lgamma(theta)\n",
        "            - torch.lgamma(x + 1)\n",
        "        )\n",
        "        return res\n",
        "\n",
        "\n",
        "    def loss(self,x):\n",
        "        zs,ls,y = self.forward(x)\n",
        "        z_mean, z_std, z = zs\n",
        "        l_mean, l_std, library = ls\n",
        "\n",
        "        mean, std = torch.zeros_like(z_mean), torch.ones_like(z_std)\n",
        "        kl_z = kl_divergence(Normal(z_mean,torch.sqrt(z_std)), Normal(mean, std)).sum(dim=1)\n",
        "\n",
        "        mean, std = self.local_l_mean*torch.ones_like(l_mean), self.local_l_std*torch.ones_like(l_std)\n",
        "        kl_l = kl_divergence(Normal(l_mean,torch.sqrt(l_std)), Normal(mean, torch.sqrt(std))).sum(dim=1)\n",
        "\n",
        "        reconst = self.reconst_error(x, mu=y, theta=torch.exp(self.theta)).sum(dim=-1)        \n",
        "        return reconst, kl_l ,kl_z"
      ],
      "metadata": {
        "id": "xWIBSs3AtBUj"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LDVAE(genes_cnt = adata.n_vars,\n",
        "              latent_dim = 20)\n",
        "model.set_local_l_mean_and_std(adata.to_df().values)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usgub80pSxzf",
        "outputId": "478f9a10-8575-4303-9b7e-885dbfc358da"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LDVAE(\n",
              "  (encoder_z): Sequential(\n",
              "    (0): Linear(in_features=17006, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_z_mean): Linear(in_features=128, out_features=20, bias=True)\n",
              "  (encoder_z_std): Linear(in_features=128, out_features=20, bias=True)\n",
              "  (encoder_l): Sequential(\n",
              "    (0): Linear(in_features=17006, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_l_mean): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (encoder_l_std): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=17006, bias=True)\n",
              "    (1): BatchNorm1d(17006, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GenesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, \n",
        "                 adata, \n",
        "                 transform=None, \n",
        "                 target_transform=None):\n",
        "        self.data = adata\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getattr__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        #if self.target_transform:\n",
        "        #    label = self.target_transform(label)\n",
        "        return data"
      ],
      "metadata": {
        "id": "lXvuywVxJOZz"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloder\n",
        "dataset = GenesDataset(adata.to_df().values)\n",
        "\n",
        "\n",
        "n_samples = len(dataset) \n",
        "train_size = int(n_samples* 0.65)\n",
        "val_size = int(n_samples * 0.15)\n",
        "test_size = n_samples - train_size - val_size \n",
        "\n",
        "dataset_train ,dataset_valid, dataset_test = \\\n",
        "        torch.utils.data.random_split(dataset, [train_size, val_size,test_size])\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=SETTING_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    dataset_valid,\n",
        "    batch_size=SETTING_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "#dataloader_test = torch.utils.data.DataLoader(\n",
        "#    dataset_test,\n",
        "#    batch_size=SETTING_BATCH_SIZE,\n",
        "#    shuffle=True\n",
        "#)"
      ],
      "metadata": {
        "id": "zHMDdp6BY4yo"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.ma.core import nonzero\n",
        "# train\n",
        "model = LDVAE(genes_cnt = adata.n_vars,\n",
        "              latent_dim = 20)\n",
        "model.set_local_l_mean_and_std(adata.to_df().values)\n",
        "model\n",
        "\n",
        "n_epochs  = 100\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002,  betas=(0.5,0.999))\n",
        "\n",
        "device = 'cuda'  if USE_CUDA else 'cpu'\n",
        "model.to(device)\n",
        "_x = nonzero\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses = []\n",
        "\n",
        "    model.train()\n",
        "    for x in dataloader_train:\n",
        "        x = x.to(device)\n",
        "        _x = x\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forawrd and loss\n",
        "        reconst, kl_l ,kl_z = model.loss(x)\n",
        "        loss = torch.mean(-reconst+kl_l*0.05 +kl_z*0.25)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    losses_val1 = []\n",
        "    losses_val2 = []\n",
        "    losses_val3 = []\n",
        "    model.eval()\n",
        "    for x in dataloader_valid:\n",
        "\n",
        "        x = x.to(device)\n",
        "\n",
        "\n",
        "        reconst, kl_l ,kl_z = model.loss(x)\n",
        "\n",
        "        losses_val1.append(torch.mean(-reconst).cpu().detach().numpy())\n",
        "        losses_val2.append(torch.mean(kl_l).cpu().detach().numpy())\n",
        "        losses_val3.append(torch.mean(kl_z).cpu().detach().numpy())\n",
        "\n",
        "    print('EPOCH: %d    Train Loss: %lf    Valid rec: %lf    Valid kl_l: %lf    Valid kl_z: %lf' %\n",
        "            (epoch+1, np.average(losses),np.average(losses_val1),np.average(losses_val2),np.average(losses_val3)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D6vYBA6icfn",
        "outputId": "1d5d57e9-5ba6-4163-8700-9318f033edc0"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1    Train Loss: 100572.148438    Valid rec: 70945.664062    Valid kl_l: 62.383652    Valid kl_z: 127.055717\n",
            "EPOCH: 2    Train Loss: 32966.945312    Valid rec: 45180.191406    Valid kl_l: 22.319647    Valid kl_z: 149.894180\n",
            "EPOCH: 3    Train Loss: 22126.449219    Valid rec: 26281.093750    Valid kl_l: 11.950855    Valid kl_z: 102.500130\n",
            "EPOCH: 4    Train Loss: 20578.800781    Valid rec: 24529.875000    Valid kl_l: 16.179651    Valid kl_z: 76.694832\n",
            "EPOCH: 5    Train Loss: 19319.636719    Valid rec: 20130.355469    Valid kl_l: 8.774171    Valid kl_z: 70.535065\n",
            "EPOCH: 6    Train Loss: 18138.105469    Valid rec: 25518.388672    Valid kl_l: 13.519912    Valid kl_z: 68.894867\n",
            "EPOCH: 7    Train Loss: 17392.591797    Valid rec: 19699.373047    Valid kl_l: 6.785427    Valid kl_z: 70.373444\n",
            "EPOCH: 8    Train Loss: 16715.814453    Valid rec: 18464.527344    Valid kl_l: 5.858596    Valid kl_z: 73.331337\n",
            "EPOCH: 9    Train Loss: 16088.311523    Valid rec: 15674.473633    Valid kl_l: 4.139520    Valid kl_z: 71.798553\n",
            "EPOCH: 10    Train Loss: 15401.735352    Valid rec: 15427.797852    Valid kl_l: 4.065741    Valid kl_z: 66.991829\n",
            "EPOCH: 11    Train Loss: 14978.394531    Valid rec: 14364.032227    Valid kl_l: 4.241136    Valid kl_z: 61.907085\n",
            "EPOCH: 12    Train Loss: 14555.525391    Valid rec: 16028.155273    Valid kl_l: 5.698873    Valid kl_z: 64.672356\n",
            "EPOCH: 13    Train Loss: 14114.882812    Valid rec: 13647.911133    Valid kl_l: 4.120599    Valid kl_z: 72.442314\n",
            "EPOCH: 14    Train Loss: 13727.476562    Valid rec: 13879.359375    Valid kl_l: 4.854664    Valid kl_z: 71.995201\n",
            "EPOCH: 15    Train Loss: 13442.046875    Valid rec: 13213.710938    Valid kl_l: 3.928963    Valid kl_z: 67.262749\n",
            "EPOCH: 16    Train Loss: 13153.112305    Valid rec: 13029.656250    Valid kl_l: 4.320510    Valid kl_z: 63.173630\n",
            "EPOCH: 17    Train Loss: 12817.415039    Valid rec: 12921.520508    Valid kl_l: 4.036879    Valid kl_z: 61.161926\n",
            "EPOCH: 18    Train Loss: 12700.349609    Valid rec: 13124.063477    Valid kl_l: 4.783141    Valid kl_z: 64.164658\n",
            "EPOCH: 19    Train Loss: 12508.859375    Valid rec: 12732.047852    Valid kl_l: 4.358703    Valid kl_z: 64.972054\n",
            "EPOCH: 20    Train Loss: 12269.061523    Valid rec: 12498.813477    Valid kl_l: 4.946951    Valid kl_z: 71.752785\n",
            "EPOCH: 21    Train Loss: 12097.866211    Valid rec: 11843.936523    Valid kl_l: 3.972949    Valid kl_z: 59.030090\n",
            "EPOCH: 22    Train Loss: 11887.247070    Valid rec: 11856.292969    Valid kl_l: 4.562918    Valid kl_z: 57.996456\n",
            "EPOCH: 23    Train Loss: 11747.517578    Valid rec: 11762.914062    Valid kl_l: 4.207951    Valid kl_z: 56.854404\n",
            "EPOCH: 24    Train Loss: 11608.570312    Valid rec: 11559.274414    Valid kl_l: 4.101832    Valid kl_z: 58.924595\n",
            "EPOCH: 25    Train Loss: 11514.480469    Valid rec: 11268.198242    Valid kl_l: 3.958080    Valid kl_z: 55.099880\n",
            "EPOCH: 26    Train Loss: 11417.492188    Valid rec: 11567.924805    Valid kl_l: 4.353156    Valid kl_z: 58.949127\n",
            "EPOCH: 27    Train Loss: 11274.532227    Valid rec: 11326.176758    Valid kl_l: 4.760022    Valid kl_z: 61.003632\n",
            "EPOCH: 28    Train Loss: 11199.549805    Valid rec: 11020.639648    Valid kl_l: 4.567498    Valid kl_z: 59.719265\n",
            "EPOCH: 29    Train Loss: 11093.495117    Valid rec: 10965.989258    Valid kl_l: 4.277793    Valid kl_z: 51.731934\n",
            "EPOCH: 30    Train Loss: 11028.095703    Valid rec: 10917.942383    Valid kl_l: 4.309821    Valid kl_z: 57.331356\n",
            "EPOCH: 31    Train Loss: 10933.671875    Valid rec: 11161.441406    Valid kl_l: 5.395732    Valid kl_z: 58.046513\n",
            "EPOCH: 32    Train Loss: 10864.407227    Valid rec: 10889.418945    Valid kl_l: 4.345849    Valid kl_z: 49.447865\n",
            "EPOCH: 33    Train Loss: 10787.350586    Valid rec: 10928.335938    Valid kl_l: 4.625041    Valid kl_z: 45.991352\n",
            "EPOCH: 34    Train Loss: 10720.426758    Valid rec: 10626.250977    Valid kl_l: 4.476991    Valid kl_z: 55.779484\n",
            "EPOCH: 35    Train Loss: 10647.183594    Valid rec: 11760.887695    Valid kl_l: 5.268918    Valid kl_z: 47.709446\n",
            "EPOCH: 36    Train Loss: 10593.226562    Valid rec: 10955.097656    Valid kl_l: 4.665593    Valid kl_z: 52.450512\n",
            "EPOCH: 37    Train Loss: 10531.870117    Valid rec: 10843.635742    Valid kl_l: 5.632124    Valid kl_z: 50.983334\n",
            "EPOCH: 38    Train Loss: 10471.099609    Valid rec: 10489.342773    Valid kl_l: 4.734215    Valid kl_z: 49.897232\n",
            "EPOCH: 39    Train Loss: 10426.331055    Valid rec: 10779.821289    Valid kl_l: 5.522820    Valid kl_z: 55.277462\n",
            "EPOCH: 40    Train Loss: 10401.958008    Valid rec: 12065.809570    Valid kl_l: 6.020696    Valid kl_z: 53.897842\n",
            "EPOCH: 41    Train Loss: 10394.688477    Valid rec: 10557.955078    Valid kl_l: 4.912726    Valid kl_z: 55.488621\n",
            "EPOCH: 42    Train Loss: 10299.943359    Valid rec: 10289.215820    Valid kl_l: 4.580699    Valid kl_z: 52.484402\n",
            "EPOCH: 43    Train Loss: 10282.433594    Valid rec: 10161.319336    Valid kl_l: 4.499486    Valid kl_z: 54.233093\n",
            "EPOCH: 44    Train Loss: 10196.928711    Valid rec: 10584.235352    Valid kl_l: 4.708088    Valid kl_z: 50.289108\n",
            "EPOCH: 45    Train Loss: 10184.729492    Valid rec: 10700.872070    Valid kl_l: 5.029994    Valid kl_z: 56.269928\n",
            "EPOCH: 46    Train Loss: 10149.599609    Valid rec: 11340.226562    Valid kl_l: 5.521386    Valid kl_z: 49.399227\n",
            "EPOCH: 47    Train Loss: 10147.976562    Valid rec: 10355.513672    Valid kl_l: 5.628612    Valid kl_z: 52.039093\n",
            "EPOCH: 48    Train Loss: 10120.581055    Valid rec: 10155.443359    Valid kl_l: 5.047719    Valid kl_z: 53.483917\n",
            "EPOCH: 49    Train Loss: 10054.739258    Valid rec: 10116.507812    Valid kl_l: 4.712756    Valid kl_z: 53.964233\n",
            "EPOCH: 50    Train Loss: 10048.590820    Valid rec: 16159.915039    Valid kl_l: 10.637962    Valid kl_z: 49.197567\n",
            "EPOCH: 51    Train Loss: 10023.553711    Valid rec: 10483.301758    Valid kl_l: 4.870602    Valid kl_z: 41.284855\n",
            "EPOCH: 52    Train Loss: 10030.089844    Valid rec: 10159.137695    Valid kl_l: 5.149806    Valid kl_z: 57.485096\n",
            "EPOCH: 53    Train Loss: 9931.919922    Valid rec: 9814.531250    Valid kl_l: 4.632369    Valid kl_z: 52.644306\n",
            "EPOCH: 54    Train Loss: 9903.616211    Valid rec: 9897.914062    Valid kl_l: 4.599401    Valid kl_z: 48.602222\n",
            "EPOCH: 55    Train Loss: 9954.094727    Valid rec: 10119.750000    Valid kl_l: 5.202159    Valid kl_z: 60.035259\n",
            "EPOCH: 56    Train Loss: 9891.736328    Valid rec: 9715.129883    Valid kl_l: 4.823353    Valid kl_z: 56.635281\n",
            "EPOCH: 57    Train Loss: 9836.780273    Valid rec: 9781.643555    Valid kl_l: 4.848098    Valid kl_z: 58.161633\n",
            "EPOCH: 58    Train Loss: 9820.444336    Valid rec: 9705.192383    Valid kl_l: 4.681170    Valid kl_z: 53.830242\n",
            "EPOCH: 59    Train Loss: 9826.755859    Valid rec: 10127.292969    Valid kl_l: 5.137577    Valid kl_z: 57.074051\n",
            "EPOCH: 60    Train Loss: 9783.132812    Valid rec: 10276.992188    Valid kl_l: 5.838820    Valid kl_z: 50.939045\n",
            "EPOCH: 61    Train Loss: 9784.930664    Valid rec: 9791.322266    Valid kl_l: 4.872976    Valid kl_z: 60.184956\n",
            "EPOCH: 62    Train Loss: 9772.511719    Valid rec: 9781.201172    Valid kl_l: 4.750878    Valid kl_z: 52.032619\n",
            "EPOCH: 63    Train Loss: 9781.958008    Valid rec: 10428.180664    Valid kl_l: 6.251918    Valid kl_z: 55.602329\n",
            "EPOCH: 64    Train Loss: 9720.732422    Valid rec: 9719.749023    Valid kl_l: 5.190340    Valid kl_z: 50.194946\n",
            "EPOCH: 65    Train Loss: 9692.635742    Valid rec: 9863.965820    Valid kl_l: 5.374647    Valid kl_z: 59.298431\n",
            "EPOCH: 66    Train Loss: 9708.494141    Valid rec: 9538.149414    Valid kl_l: 4.950320    Valid kl_z: 53.132313\n",
            "EPOCH: 67    Train Loss: 9661.800781    Valid rec: 9534.059570    Valid kl_l: 5.027825    Valid kl_z: 49.585346\n",
            "EPOCH: 68    Train Loss: 9626.255859    Valid rec: 9764.277344    Valid kl_l: 5.273299    Valid kl_z: 50.795063\n",
            "EPOCH: 69    Train Loss: 9635.652344    Valid rec: 10208.681641    Valid kl_l: 5.317257    Valid kl_z: 50.266254\n",
            "EPOCH: 70    Train Loss: 9600.304688    Valid rec: 9554.635742    Valid kl_l: 5.085467    Valid kl_z: 48.643650\n",
            "EPOCH: 71    Train Loss: 9623.808594    Valid rec: 9533.413086    Valid kl_l: 4.861928    Valid kl_z: 47.543636\n",
            "EPOCH: 72    Train Loss: 9621.144531    Valid rec: 10377.240234    Valid kl_l: 5.650108    Valid kl_z: 48.261841\n",
            "EPOCH: 73    Train Loss: 9605.262695    Valid rec: 9923.186523    Valid kl_l: 5.679210    Valid kl_z: 53.049286\n",
            "EPOCH: 74    Train Loss: 9562.563477    Valid rec: 9497.868164    Valid kl_l: 5.202817    Valid kl_z: 52.776714\n",
            "EPOCH: 75    Train Loss: 9559.098633    Valid rec: 9502.058594    Valid kl_l: 5.215899    Valid kl_z: 55.429016\n",
            "EPOCH: 76    Train Loss: 9579.875977    Valid rec: 9360.242188    Valid kl_l: 4.963312    Valid kl_z: 52.564884\n",
            "EPOCH: 77    Train Loss: 9537.073242    Valid rec: 9388.200195    Valid kl_l: 5.038025    Valid kl_z: 48.914257\n",
            "EPOCH: 78    Train Loss: 9521.235352    Valid rec: 9325.004883    Valid kl_l: 4.854880    Valid kl_z: 52.887577\n",
            "EPOCH: 79    Train Loss: 9511.434570    Valid rec: 9610.871094    Valid kl_l: 5.455721    Valid kl_z: 56.303150\n",
            "EPOCH: 80    Train Loss: 9481.190430    Valid rec: 9387.493164    Valid kl_l: 5.076441    Valid kl_z: 56.458233\n",
            "EPOCH: 81    Train Loss: 9500.049805    Valid rec: 9379.709961    Valid kl_l: 5.208207    Valid kl_z: 52.872227\n",
            "EPOCH: 82    Train Loss: 9490.943359    Valid rec: 9489.756836    Valid kl_l: 5.213506    Valid kl_z: 52.636181\n",
            "EPOCH: 83    Train Loss: 9439.829102    Valid rec: 10331.408203    Valid kl_l: 5.811426    Valid kl_z: 45.939541\n",
            "EPOCH: 84    Train Loss: 9438.943359    Valid rec: 9317.650391    Valid kl_l: 4.925488    Valid kl_z: 52.872665\n",
            "EPOCH: 85    Train Loss: 9439.548828    Valid rec: 9307.261719    Valid kl_l: 4.934000    Valid kl_z: 54.955746\n",
            "EPOCH: 86    Train Loss: 9425.924805    Valid rec: 10536.208984    Valid kl_l: 6.040261    Valid kl_z: 53.808727\n",
            "EPOCH: 87    Train Loss: 9420.758789    Valid rec: 9558.758789    Valid kl_l: 5.217038    Valid kl_z: 49.813717\n",
            "EPOCH: 88    Train Loss: 9408.146484    Valid rec: 9243.906250    Valid kl_l: 4.989395    Valid kl_z: 48.498119\n",
            "EPOCH: 89    Train Loss: 9416.093750    Valid rec: 9230.965820    Valid kl_l: 4.898583    Valid kl_z: 54.900532\n",
            "EPOCH: 90    Train Loss: 9392.716797    Valid rec: 9198.519531    Valid kl_l: 4.993605    Valid kl_z: 52.819614\n",
            "EPOCH: 91    Train Loss: 9361.011719    Valid rec: 9781.193359    Valid kl_l: 5.659287    Valid kl_z: 52.722229\n",
            "EPOCH: 92    Train Loss: 9354.717773    Valid rec: 9237.397461    Valid kl_l: 4.837050    Valid kl_z: 53.626736\n",
            "EPOCH: 93    Train Loss: 9382.500977    Valid rec: 9245.104492    Valid kl_l: 5.093260    Valid kl_z: 47.048725\n",
            "EPOCH: 94    Train Loss: 9342.785156    Valid rec: 9176.834961    Valid kl_l: 5.012602    Valid kl_z: 50.518993\n",
            "EPOCH: 95    Train Loss: 9321.040039    Valid rec: 9166.757812    Valid kl_l: 4.839565    Valid kl_z: 55.864094\n",
            "EPOCH: 96    Train Loss: 9329.670898    Valid rec: 9153.812500    Valid kl_l: 4.990574    Valid kl_z: 50.002823\n",
            "EPOCH: 97    Train Loss: 9304.834961    Valid rec: 9290.183594    Valid kl_l: 4.988143    Valid kl_z: 49.383785\n",
            "EPOCH: 98    Train Loss: 9298.749023    Valid rec: 9176.938477    Valid kl_l: 4.921608    Valid kl_z: 50.847706\n",
            "EPOCH: 99    Train Loss: 9311.406250    Valid rec: 9105.021484    Valid kl_l: 4.885332    Valid kl_z: 55.315536\n",
            "EPOCH: 100    Train Loss: 9281.547852    Valid rec: 9716.766602    Valid kl_l: 5.511261    Valid kl_z: 48.104618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KZECrprtdiCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}