{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDVAE.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOn7/EnX3rgUPoS5aV3tnm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kicysh/final_task_of_world_model_lecture_2021/blob/main/src/LDVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip"
      ],
      "metadata": {
        "id": "mF-ceB4dTt1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy scikit-misc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2EsievTT474",
        "outputId": "c1127c32-2271-4cee-8be1-f16c436b0027"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.7/dist-packages (1.8.2)\n",
            "Requirement already satisfied: scikit-misc in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: sinfo in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.3.4)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.63.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.7.0)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.11.3)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.6)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.7/dist-packages (from sinfo->scanpy) (0.8.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "kZnO-gmJTTwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!gsutil cp gs://h5ad/2019-02-Pijuan-Sala-et-al-Nature/pijuan_sala_atlas.h5ad /content/data\n",
        "path_of_data = '/content/data/pijuan_sala_atlas.h5ad'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCBTOxD6Tg7h",
        "outputId": "e69b1529-090c-431d-fd2c-c2ee709410a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Copying gs://h5ad/2019-02-Pijuan-Sala-et-al-Nature/pijuan_sala_atlas.h5ad...\n",
            "\\ [1 files][  1.0 GiB/  1.0 GiB]   46.6 MiB/s                                   \n",
            "Operation completed over 1 objects/1.0 GiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setting\n"
      ],
      "metadata": {
        "id": "47p9z8QQs4bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting\n",
        "SETTING_BATCHNORM_EPS = 0.001\n",
        "SETTING_BATCHNORM_MOMENTUM = 0.01\n",
        "SETTING_ENCODER_Z_DROPOUT_P = 0.1\n",
        "SETTING_ENCODER_L_DROPOUT_P = 0.1\n",
        "SETTING_HIDDEN_DIM = 128\n",
        "SETTING_EPS = 1e-8\n",
        "\n",
        "USE_CUDA = True\n",
        "SETTING_BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "grS7L_2ms24J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "Q0rzv6RVs9sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HFG8utJ7LYQm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import logsumexp\n",
        "from torch.distributions import Normal, kl_divergence\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(1234)\n",
        "random_state = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "adata = sc.read_h5ad(path_of_data)\n",
        "adata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BzFALOqUJ-j",
        "outputId": "5e027a54-4798-4030-dd10-639d89e93a3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 139331 × 29452\n",
              "    obs: 'barcode', 'sample', 'stage', 'sequencing.batch', 'theiler', 'doub.density', 'doublet', 'cluster', 'cluster.sub', 'cluster.stage', 'cluster.theiler', 'stripped', 'celltype', 'colour', 'umapX', 'umapY', 'haem_gephiX', 'haem_gephiY', 'haem_subclust', 'endo_gephiX', 'endo_gephiY', 'endo_trajectoryName', 'endo_trajectoryDPT', 'endo_gutX', 'endo_gutY', 'endo_gutDPT', 'endo_gutCluster'\n",
              "    var: 'gene_name'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = adata.obs.query('not doublet').index\n",
        "idx = np.random.choice(idx, 20000, replace=False)\n",
        "adata = adata[idx]\n",
        "sc.pp.highly_variable_genes(adata,n_top_genes=1000 ,flavor='seurat_v3')\n",
        "print(adata)\n",
        "#sc.pp.filter_genes(adata, min_cells=100)\n"
      ],
      "metadata": {
        "id": "I81c3-hrQeL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f48afd3-5d9d-4aa6-e070-67a96737c281"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/anndata/compat/_overloaded_dict.py:106: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.\n",
            "  self.data[key] = value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 20000 × 29452\n",
            "    obs: 'barcode', 'sample', 'stage', 'sequencing.batch', 'theiler', 'doub.density', 'doublet', 'cluster', 'cluster.sub', 'cluster.stage', 'cluster.theiler', 'stripped', 'celltype', 'colour', 'umapX', 'umapY', 'haem_gephiX', 'haem_gephiY', 'haem_subclust', 'endo_gephiX', 'endo_gephiY', 'endo_trajectoryName', 'endo_trajectoryDPT', 'endo_gutX', 'endo_gutY', 'endo_gutDPT', 'endo_gutCluster'\n",
            "    var: 'gene_name', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
            "    uns: 'hvg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gene_index = adata.var[adata.var['highly_variable']].index"
      ],
      "metadata": {
        "id": "eQPFDZIQlJ2e"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata_df = adata.to_df()[gene_index]"
      ],
      "metadata": {
        "id": "swnh7e6boEMV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from math import ldexp\n",
        "\n",
        "class LDVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    :param genes_cnt: Number of input genes\n",
        "    :param latent_dim: Dimensionality of the latent space \n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        genes_cnt: int, \n",
        "        latent_dim: int = 20\n",
        "    ):\n",
        "        super(LDVAE,self).__init__()\n",
        "        self.local_l_mean = None\n",
        "        self.local_l_std = None\n",
        "        self.eps = SETTING_EPS\n",
        "\n",
        "        self.theta = nn.Parameter(torch.randn(genes_cnt))\n",
        "        self.encoder_z = nn.Sequential(\n",
        "            nn.Linear(genes_cnt, SETTING_HIDDEN_DIM),\n",
        "            nn.BatchNorm1d(SETTING_HIDDEN_DIM,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(SETTING_ENCODER_Z_DROPOUT_P)\n",
        "        )\n",
        "        self.encoder_z_mean = nn.Linear(SETTING_HIDDEN_DIM,latent_dim)\n",
        "        self.encoder_z_std = nn.Linear(SETTING_HIDDEN_DIM,latent_dim)\n",
        "\n",
        "        self.encoder_l = nn.Sequential(\n",
        "            nn.Linear(genes_cnt, SETTING_HIDDEN_DIM),\n",
        "            nn.BatchNorm1d(SETTING_HIDDEN_DIM,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(SETTING_ENCODER_L_DROPOUT_P)\n",
        "        )\n",
        "        self.encoder_l_mean = nn.Linear(SETTING_HIDDEN_DIM, 1)\n",
        "        self.encoder_l_std = nn.Linear(SETTING_HIDDEN_DIM, 1)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, genes_cnt),\n",
        "            nn.BatchNorm1d(genes_cnt,\n",
        "                           eps=SETTING_BATCHNORM_EPS, \n",
        "                           momentum=SETTING_BATCHNORM_MOMENTUM)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_z = self.encoder_z(x)\n",
        "        z_mean = self.encoder_z_mean(x_z)\n",
        "        z_std = torch.exp(self.encoder_z_std(x_z)) \n",
        "        z = Normal(z_mean, z_std.sqrt()).rsample()\n",
        "\n",
        "        x_l = self.encoder_l(x)\n",
        "        l_mean = self.encoder_l_mean(x_l)\n",
        "        l_std = torch.exp(self.encoder_l_std(x_l))\n",
        "        library = Normal(l_mean, l_std.sqrt()).rsample()\n",
        "\n",
        "        y = self.decoder(z)\n",
        "        y = torch.exp(library)*torch.softmax(y, dim=-1)\n",
        "        return [z_mean, z_std, z], [l_mean, l_std, library], y\n",
        "\n",
        "\n",
        "    def set_local_l_mean_and_std(self, data):\n",
        "        masked_log_sum =np.ma.log(data.sum(axis=1))\n",
        "        log_counts = masked_log_sum.filled(0)\n",
        "        self.local_l_mean = (np.mean(log_counts).reshape(-1, 1)).astype(np.float32)[0][0]\n",
        "        self.local_l_std = (np.var(log_counts).reshape(-1, 1)).astype(np.float32)[0][0]\n",
        "        return self.local_l_mean, self.local_l_std\n",
        "\n",
        "\n",
        "    def reconst_error(self,x, mu, theta):\n",
        "        eps = SETTING_EPS\n",
        "        log_theta_mu_eps = torch.log(theta + mu + eps)\n",
        "\n",
        "        res = (\n",
        "            theta * (torch.log(theta + eps) - log_theta_mu_eps)\n",
        "            + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
        "            + torch.lgamma(x + theta)\n",
        "            - torch.lgamma(theta)\n",
        "            - torch.lgamma(x + 1)\n",
        "        )\n",
        "        return res\n",
        "\n",
        "\n",
        "    def loss(self,x):\n",
        "        zs,ls,y = self.forward(x)\n",
        "        z_mean, z_std, z = zs\n",
        "        l_mean, l_std, library = ls\n",
        "\n",
        "        mean, std = torch.zeros_like(z_mean), torch.ones_like(z_std)\n",
        "        kl_z = kl_divergence(Normal(z_mean,torch.sqrt(z_std)), Normal(mean, std)).sum(dim=1)\n",
        "\n",
        "        mean, std = self.local_l_mean*torch.ones_like(l_mean), self.local_l_std*torch.ones_like(l_std)\n",
        "        kl_l = kl_divergence(Normal(l_mean,torch.sqrt(l_std)), Normal(mean, torch.sqrt(std))).sum(dim=1)\n",
        "\n",
        "        reconst = self.reconst_error(x, mu=y, theta=torch.exp(self.theta)).sum(dim=-1)        \n",
        "        return reconst, kl_l ,kl_z"
      ],
      "metadata": {
        "id": "xWIBSs3AtBUj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LDVAE(genes_cnt = len(adata_df.columns),\n",
        "              latent_dim = 20)\n",
        "model.set_local_l_mean_and_std(adata_df.values)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usgub80pSxzf",
        "outputId": "d96d9adf-5825-48f1-ba91-c383953835a5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LDVAE(\n",
              "  (encoder_z): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_z_mean): Linear(in_features=128, out_features=20, bias=True)\n",
              "  (encoder_z_std): Linear(in_features=128, out_features=20, bias=True)\n",
              "  (encoder_l): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_l_mean): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (encoder_l_std): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=1000, bias=True)\n",
              "    (1): BatchNorm1d(1000, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GenesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, \n",
        "                 adata, \n",
        "                 transform=None, \n",
        "                 target_transform=None):\n",
        "        self.data = adata\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getattr__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        #if self.target_transform:\n",
        "        #    label = self.target_transform(label)\n",
        "        return data"
      ],
      "metadata": {
        "id": "lXvuywVxJOZz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloder\n",
        "dataset = GenesDataset(adata_df.values)\n",
        "\n",
        "\n",
        "n_samples = len(dataset) \n",
        "train_size = int(n_samples* 0.65)\n",
        "val_size = int(n_samples * 0.15)\n",
        "test_size = n_samples - train_size - val_size \n",
        "\n",
        "dataset_train ,dataset_valid, dataset_test = \\\n",
        "        torch.utils.data.random_split(dataset, [train_size, val_size,test_size])\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=SETTING_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    dataset_valid,\n",
        "    batch_size=SETTING_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "#dataloader_test = torch.utils.data.DataLoader(\n",
        "#    dataset_test,\n",
        "#    batch_size=SETTING_BATCH_SIZE,\n",
        "#    shuffle=True\n",
        "#)"
      ],
      "metadata": {
        "id": "zHMDdp6BY4yo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.ma.core import nonzero\n",
        "# train\n",
        "model = LDVAE(genes_cnt = len(adata_df.columns),\n",
        "              latent_dim = 20)\n",
        "model.set_local_l_mean_and_std(adata_df.values)\n",
        "model\n",
        "\n",
        "n_epochs  = 100\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002,  betas=(0.5,0.999))\n",
        "\n",
        "device = 'cuda'  if USE_CUDA else 'cpu'\n",
        "model.to(device)\n",
        "_x = nonzero\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses = []\n",
        "\n",
        "    model.train()\n",
        "    for x in dataloader_train:\n",
        "        x = x.to(device)\n",
        "        _x = x\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forawrd and loss\n",
        "        reconst, kl_l ,kl_z = model.loss(x)\n",
        "        loss = torch.mean(-reconst+kl_l*0.05 +kl_z*0.25)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    losses_val1 = []\n",
        "    losses_val2 = []\n",
        "    losses_val3 = []\n",
        "    model.eval()\n",
        "    for x in dataloader_valid:\n",
        "\n",
        "        x = x.to(device)\n",
        "\n",
        "\n",
        "        reconst, kl_l ,kl_z = model.loss(x)\n",
        "\n",
        "        losses_val1.append(torch.mean(-reconst).cpu().detach().numpy())\n",
        "        losses_val2.append(torch.mean(kl_l).cpu().detach().numpy())\n",
        "        losses_val3.append(torch.mean(kl_z).cpu().detach().numpy())\n",
        "\n",
        "    print('EPOCH: %d    Train Loss: %lf    Valid rec: %lf    Valid kl_l: %lf    Valid kl_z: %lf' %\n",
        "            (epoch+1, np.average(losses),np.average(losses_val1),np.average(losses_val2),np.average(losses_val3)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D6vYBA6icfn",
        "outputId": "cda54094-0cee-4020-bf90-bd1356340b9e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1    Train Loss: 3943.533203    Valid rec: 1935.536743    Valid kl_l: 12.102735    Valid kl_z: 52.642838\n",
            "EPOCH: 2    Train Loss: 1370.984985    Valid rec: 1326.448364    Valid kl_l: 6.463783    Valid kl_z: 29.813789\n",
            "EPOCH: 3    Train Loss: 1133.139648    Valid rec: 1163.127808    Valid kl_l: 3.669250    Valid kl_z: 28.958839\n",
            "EPOCH: 4    Train Loss: 1046.825806    Valid rec: 980.517151    Valid kl_l: 2.604321    Valid kl_z: 33.954975\n",
            "EPOCH: 5    Train Loss: 998.076294    Valid rec: 967.799561    Valid kl_l: 3.338364    Valid kl_z: 37.366375\n",
            "EPOCH: 6    Train Loss: 954.183105    Valid rec: 972.898865    Valid kl_l: 3.699333    Valid kl_z: 36.882259\n",
            "EPOCH: 7    Train Loss: 914.192017    Valid rec: 858.116699    Valid kl_l: 2.701347    Valid kl_z: 37.534718\n",
            "EPOCH: 8    Train Loss: 880.983215    Valid rec: 814.440735    Valid kl_l: 2.706646    Valid kl_z: 37.524574\n",
            "EPOCH: 9    Train Loss: 855.980530    Valid rec: 800.635986    Valid kl_l: 2.544543    Valid kl_z: 38.962627\n",
            "EPOCH: 10    Train Loss: 829.329285    Valid rec: 791.944885    Valid kl_l: 3.003467    Valid kl_z: 36.622311\n",
            "EPOCH: 11    Train Loss: 808.299255    Valid rec: 770.027527    Valid kl_l: 2.896732    Valid kl_z: 38.035519\n",
            "EPOCH: 12    Train Loss: 788.320557    Valid rec: 739.683411    Valid kl_l: 2.934006    Valid kl_z: 39.834984\n",
            "EPOCH: 13    Train Loss: 771.154114    Valid rec: 728.935974    Valid kl_l: 2.886470    Valid kl_z: 35.925755\n",
            "EPOCH: 14    Train Loss: 758.705750    Valid rec: 745.796936    Valid kl_l: 3.631294    Valid kl_z: 38.909794\n",
            "EPOCH: 15    Train Loss: 742.697693    Valid rec: 706.026123    Valid kl_l: 2.978137    Valid kl_z: 38.204769\n",
            "EPOCH: 16    Train Loss: 729.074158    Valid rec: 697.727722    Valid kl_l: 3.074673    Valid kl_z: 39.017994\n",
            "EPOCH: 17    Train Loss: 718.544067    Valid rec: 728.562988    Valid kl_l: 3.845185    Valid kl_z: 36.825733\n",
            "EPOCH: 18    Train Loss: 707.979980    Valid rec: 671.312805    Valid kl_l: 3.158841    Valid kl_z: 36.874802\n",
            "EPOCH: 19    Train Loss: 700.483521    Valid rec: 666.944641    Valid kl_l: 3.380755    Valid kl_z: 37.556866\n",
            "EPOCH: 20    Train Loss: 688.721130    Valid rec: 660.413879    Valid kl_l: 3.250971    Valid kl_z: 37.265926\n",
            "EPOCH: 21    Train Loss: 682.638733    Valid rec: 664.687927    Valid kl_l: 3.201091    Valid kl_z: 37.874081\n",
            "EPOCH: 22    Train Loss: 674.552612    Valid rec: 645.466064    Valid kl_l: 3.458935    Valid kl_z: 39.221485\n",
            "EPOCH: 23    Train Loss: 665.795654    Valid rec: 638.257385    Valid kl_l: 3.408190    Valid kl_z: 37.014824\n",
            "EPOCH: 24    Train Loss: 660.609070    Valid rec: 636.406067    Valid kl_l: 3.436786    Valid kl_z: 35.957645\n",
            "EPOCH: 25    Train Loss: 655.772888    Valid rec: 632.838013    Valid kl_l: 3.586039    Valid kl_z: 36.817150\n",
            "EPOCH: 26    Train Loss: 650.966614    Valid rec: 620.944763    Valid kl_l: 3.576737    Valid kl_z: 37.236591\n",
            "EPOCH: 27    Train Loss: 645.680298    Valid rec: 620.373718    Valid kl_l: 3.627784    Valid kl_z: 36.443405\n",
            "EPOCH: 28    Train Loss: 639.942871    Valid rec: 615.309448    Valid kl_l: 3.649860    Valid kl_z: 36.603481\n",
            "EPOCH: 29    Train Loss: 634.994934    Valid rec: 606.994690    Valid kl_l: 3.591876    Valid kl_z: 36.668236\n",
            "EPOCH: 30    Train Loss: 630.785706    Valid rec: 603.492249    Valid kl_l: 3.558373    Valid kl_z: 36.085720\n",
            "EPOCH: 31    Train Loss: 625.631348    Valid rec: 609.973083    Valid kl_l: 3.740168    Valid kl_z: 36.440556\n",
            "EPOCH: 32    Train Loss: 623.962646    Valid rec: 607.239563    Valid kl_l: 3.832485    Valid kl_z: 35.519035\n",
            "EPOCH: 33    Train Loss: 620.514343    Valid rec: 602.492737    Valid kl_l: 3.600878    Valid kl_z: 36.652958\n",
            "EPOCH: 34    Train Loss: 616.822205    Valid rec: 589.840271    Valid kl_l: 3.734498    Valid kl_z: 35.564167\n",
            "EPOCH: 35    Train Loss: 615.888855    Valid rec: 586.873474    Valid kl_l: 3.775019    Valid kl_z: 36.447357\n",
            "EPOCH: 36    Train Loss: 609.539978    Valid rec: 587.245911    Valid kl_l: 3.634957    Valid kl_z: 34.779636\n",
            "EPOCH: 37    Train Loss: 608.647156    Valid rec: 588.315552    Valid kl_l: 3.857794    Valid kl_z: 35.999535\n",
            "EPOCH: 38    Train Loss: 605.167786    Valid rec: 578.943787    Valid kl_l: 3.784633    Valid kl_z: 37.144730\n",
            "EPOCH: 39    Train Loss: 603.843201    Valid rec: 595.843079    Valid kl_l: 4.061578    Valid kl_z: 36.950336\n",
            "EPOCH: 40    Train Loss: 601.926636    Valid rec: 574.134644    Valid kl_l: 3.797005    Valid kl_z: 36.371410\n",
            "EPOCH: 41    Train Loss: 599.042786    Valid rec: 573.090210    Valid kl_l: 3.897639    Valid kl_z: 36.458309\n",
            "EPOCH: 42    Train Loss: 597.549438    Valid rec: 605.351074    Valid kl_l: 3.688218    Valid kl_z: 36.028637\n",
            "EPOCH: 43    Train Loss: 594.885193    Valid rec: 572.436401    Valid kl_l: 3.852830    Valid kl_z: 35.978451\n",
            "EPOCH: 44    Train Loss: 591.526245    Valid rec: 566.186523    Valid kl_l: 3.861126    Valid kl_z: 35.490185\n",
            "EPOCH: 45    Train Loss: 600.929626    Valid rec: 568.096375    Valid kl_l: 3.916080    Valid kl_z: 39.263103\n",
            "EPOCH: 46    Train Loss: 592.252197    Valid rec: 565.201538    Valid kl_l: 3.869509    Valid kl_z: 37.094357\n",
            "EPOCH: 47    Train Loss: 589.069641    Valid rec: 564.818176    Valid kl_l: 3.986460    Valid kl_z: 38.149426\n",
            "EPOCH: 48    Train Loss: 586.516968    Valid rec: 558.587219    Valid kl_l: 3.910832    Valid kl_z: 37.676903\n",
            "EPOCH: 49    Train Loss: 583.274536    Valid rec: 563.218201    Valid kl_l: 4.038104    Valid kl_z: 34.806576\n",
            "EPOCH: 50    Train Loss: 586.465454    Valid rec: 560.497864    Valid kl_l: 3.868695    Valid kl_z: 35.640377\n",
            "EPOCH: 51    Train Loss: 585.648071    Valid rec: 557.255798    Valid kl_l: 4.086860    Valid kl_z: 35.924301\n",
            "EPOCH: 52    Train Loss: 580.257507    Valid rec: 562.302612    Valid kl_l: 4.264013    Valid kl_z: 35.413307\n",
            "EPOCH: 53    Train Loss: 577.899109    Valid rec: 555.731750    Valid kl_l: 4.083135    Valid kl_z: 35.900280\n",
            "EPOCH: 54    Train Loss: 576.884399    Valid rec: 552.264404    Valid kl_l: 4.077246    Valid kl_z: 35.738323\n",
            "EPOCH: 55    Train Loss: 578.957397    Valid rec: 556.298828    Valid kl_l: 3.901920    Valid kl_z: 37.397770\n",
            "EPOCH: 56    Train Loss: 574.631836    Valid rec: 548.872253    Valid kl_l: 4.113857    Valid kl_z: 36.105907\n",
            "EPOCH: 57    Train Loss: 573.559265    Valid rec: 549.471008    Valid kl_l: 4.035456    Valid kl_z: 36.237591\n",
            "EPOCH: 58    Train Loss: 571.766479    Valid rec: 552.701111    Valid kl_l: 4.261307    Valid kl_z: 36.317383\n",
            "EPOCH: 59    Train Loss: 571.141113    Valid rec: 548.495300    Valid kl_l: 4.174598    Valid kl_z: 36.451328\n",
            "EPOCH: 60    Train Loss: 570.358032    Valid rec: 549.929871    Valid kl_l: 4.252950    Valid kl_z: 35.334408\n",
            "EPOCH: 61    Train Loss: 568.559509    Valid rec: 543.781189    Valid kl_l: 4.029747    Valid kl_z: 34.592266\n",
            "EPOCH: 62    Train Loss: 567.577576    Valid rec: 543.850159    Valid kl_l: 4.037817    Valid kl_z: 35.599941\n",
            "EPOCH: 63    Train Loss: 566.840027    Valid rec: 541.653870    Valid kl_l: 4.080856    Valid kl_z: 35.751209\n",
            "EPOCH: 64    Train Loss: 565.647766    Valid rec: 541.939209    Valid kl_l: 4.143945    Valid kl_z: 35.634251\n",
            "EPOCH: 65    Train Loss: 564.398926    Valid rec: 541.183228    Valid kl_l: 4.013405    Valid kl_z: 35.076157\n",
            "EPOCH: 66    Train Loss: 565.697266    Valid rec: 544.338562    Valid kl_l: 4.142986    Valid kl_z: 37.236958\n",
            "EPOCH: 67    Train Loss: 563.051941    Valid rec: 537.246033    Valid kl_l: 4.039812    Valid kl_z: 36.587135\n",
            "EPOCH: 68    Train Loss: 562.876221    Valid rec: 538.246582    Valid kl_l: 4.061208    Valid kl_z: 35.584846\n",
            "EPOCH: 69    Train Loss: 560.709167    Valid rec: 540.167419    Valid kl_l: 3.964013    Valid kl_z: 36.121796\n",
            "EPOCH: 70    Train Loss: 561.421204    Valid rec: 533.755615    Valid kl_l: 4.025866    Valid kl_z: 38.515778\n",
            "EPOCH: 71    Train Loss: 559.349670    Valid rec: 548.725769    Valid kl_l: 3.997535    Valid kl_z: 37.279213\n",
            "EPOCH: 72    Train Loss: 559.045898    Valid rec: 535.763916    Valid kl_l: 3.962364    Valid kl_z: 38.276970\n",
            "EPOCH: 73    Train Loss: 557.693115    Valid rec: 533.318115    Valid kl_l: 4.040714    Valid kl_z: 37.472485\n",
            "EPOCH: 74    Train Loss: 557.347168    Valid rec: 532.208984    Valid kl_l: 4.027581    Valid kl_z: 36.389362\n",
            "EPOCH: 75    Train Loss: 556.854919    Valid rec: 531.982117    Valid kl_l: 3.964672    Valid kl_z: 37.374928\n",
            "EPOCH: 76    Train Loss: 556.201782    Valid rec: 532.383240    Valid kl_l: 4.161323    Valid kl_z: 36.333256\n",
            "EPOCH: 77    Train Loss: 555.346191    Valid rec: 532.989197    Valid kl_l: 4.035759    Valid kl_z: 35.877369\n",
            "EPOCH: 78    Train Loss: 556.693604    Valid rec: 529.043091    Valid kl_l: 4.033336    Valid kl_z: 36.941353\n",
            "EPOCH: 79    Train Loss: 554.192139    Valid rec: 528.580872    Valid kl_l: 3.977309    Valid kl_z: 37.717136\n",
            "EPOCH: 80    Train Loss: 553.373474    Valid rec: 531.719788    Valid kl_l: 3.959077    Valid kl_z: 36.827179\n",
            "EPOCH: 81    Train Loss: 553.657471    Valid rec: 530.439758    Valid kl_l: 4.092389    Valid kl_z: 35.456760\n",
            "EPOCH: 82    Train Loss: 551.650635    Valid rec: 527.836609    Valid kl_l: 3.925802    Valid kl_z: 38.340382\n",
            "EPOCH: 83    Train Loss: 552.002441    Valid rec: 527.787537    Valid kl_l: 3.894869    Valid kl_z: 37.953663\n",
            "EPOCH: 84    Train Loss: 551.985291    Valid rec: 530.465454    Valid kl_l: 3.840188    Valid kl_z: 36.230736\n",
            "EPOCH: 85    Train Loss: 551.673462    Valid rec: 527.791504    Valid kl_l: 4.021224    Valid kl_z: 35.732388\n",
            "EPOCH: 86    Train Loss: 549.506409    Valid rec: 523.948059    Valid kl_l: 3.978665    Valid kl_z: 38.022724\n",
            "EPOCH: 87    Train Loss: 548.994934    Valid rec: 524.660217    Valid kl_l: 4.025807    Valid kl_z: 36.546841\n",
            "EPOCH: 88    Train Loss: 549.173706    Valid rec: 524.957764    Valid kl_l: 3.904971    Valid kl_z: 37.153671\n",
            "EPOCH: 89    Train Loss: 549.396545    Valid rec: 523.987244    Valid kl_l: 3.957678    Valid kl_z: 36.073566\n",
            "EPOCH: 90    Train Loss: 547.770630    Valid rec: 524.905334    Valid kl_l: 3.947028    Valid kl_z: 37.975288\n",
            "EPOCH: 91    Train Loss: 547.680908    Valid rec: 524.446594    Valid kl_l: 3.972715    Valid kl_z: 36.300739\n",
            "EPOCH: 92    Train Loss: 546.878784    Valid rec: 522.866394    Valid kl_l: 3.950842    Valid kl_z: 36.169888\n",
            "EPOCH: 93    Train Loss: 547.350281    Valid rec: 534.651917    Valid kl_l: 4.166628    Valid kl_z: 38.058044\n",
            "EPOCH: 94    Train Loss: 547.683044    Valid rec: 523.408081    Valid kl_l: 3.946552    Valid kl_z: 36.701153\n",
            "EPOCH: 95    Train Loss: 545.496277    Valid rec: 521.614258    Valid kl_l: 3.936235    Valid kl_z: 36.544727\n",
            "EPOCH: 96    Train Loss: 545.669067    Valid rec: 524.388977    Valid kl_l: 3.989792    Valid kl_z: 37.301830\n",
            "EPOCH: 97    Train Loss: 544.766479    Valid rec: 520.402832    Valid kl_l: 3.917522    Valid kl_z: 37.623367\n",
            "EPOCH: 98    Train Loss: 544.752747    Valid rec: 521.656250    Valid kl_l: 3.910918    Valid kl_z: 37.216019\n",
            "EPOCH: 99    Train Loss: 544.916992    Valid rec: 523.266479    Valid kl_l: 3.885798    Valid kl_z: 36.379101\n",
            "EPOCH: 100    Train Loss: 544.329102    Valid rec: 521.139038    Valid kl_l: 3.891960    Valid kl_z: 36.142681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "_w = model.decoder[0].weight\n",
        "bn = model.decoder[1]\n",
        "sigma = torch.sqrt(bn.running_var + bn.eps)\n",
        "gamma = bn.weight\n",
        "b = gamma / sigma\n",
        "bI = torch.diag(b)\n",
        "loadings = torch.matmul(bI, _w)\n",
        "loadings = loadings.detach().cpu().numpy()\n",
        "\n",
        "W = pd.DataFrame(loadings, index=gene_index)"
      ],
      "metadata": {
        "id": "KZECrprtdiCL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "SacOf6mh4Ci4",
        "outputId": "17e72158-e136-41f6-caa4-ed89c3b5f4de"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          0         1         2         3         4   \\\n",
              "index                                                                  \n",
              "ENSMUSG00000025902 -0.114847  0.014870 -0.748335 -0.063984 -0.144368   \n",
              "ENSMUSG00000025927 -0.049106 -0.314564 -0.106663  0.515449 -0.428855   \n",
              "ENSMUSG00000026124  0.962166 -0.021462 -0.330671 -0.619310 -0.027634   \n",
              "ENSMUSG00000026043  0.456029  0.133721  0.117041 -0.044423  0.051522   \n",
              "ENSMUSG00000045954  0.677873  0.224888 -0.042514  0.125330 -0.433727   \n",
              "...                      ...       ...       ...       ...       ...   \n",
              "ENSMUSG00000025219  0.151430  0.231536 -0.541314 -0.674195 -0.303684   \n",
              "ENSMUSG00000025068 -0.373089  0.059569 -0.029015  0.127127  0.058646   \n",
              "ENSMUSG00000064341  0.062246  0.080869 -0.046041 -0.146250  0.248021   \n",
              "ENSMUSG00000064351  0.043916  0.047881 -0.083317 -0.212095  0.296512   \n",
              "ENSMUSG00000064358  0.055735  0.034203 -0.078903 -0.094865  0.303702   \n",
              "\n",
              "                          5         6         7         8         9   \\\n",
              "index                                                                  \n",
              "ENSMUSG00000025902 -0.545247  0.159475 -0.532325  0.221236 -0.063225   \n",
              "ENSMUSG00000025927 -0.189759 -0.042427 -0.180187 -0.017321  0.513675   \n",
              "ENSMUSG00000026124 -0.103679 -0.301403 -0.658163 -0.098231 -0.417534   \n",
              "ENSMUSG00000026043 -0.259104  0.188649  0.072159  0.050188  0.447276   \n",
              "ENSMUSG00000045954 -0.706035  0.597891 -0.150792 -0.602343  0.005353   \n",
              "...                      ...       ...       ...       ...       ...   \n",
              "ENSMUSG00000025219 -0.306293 -0.231648 -0.617818  0.060072  0.422939   \n",
              "ENSMUSG00000025068  0.150118 -0.060132  0.268836 -0.053790 -0.036490   \n",
              "ENSMUSG00000064341  0.042828 -0.493345  0.127642 -0.248297  0.033964   \n",
              "ENSMUSG00000064351  0.086948 -0.551643  0.109832 -0.254271 -0.036927   \n",
              "ENSMUSG00000064358  0.153246 -0.556047  0.092062 -0.284610  0.048955   \n",
              "\n",
              "                          10        11        12        13        14  \\\n",
              "index                                                                  \n",
              "ENSMUSG00000025902 -0.833941  0.424912  0.127992 -0.549809  0.059238   \n",
              "ENSMUSG00000025927  0.607979 -0.400233 -0.130607  0.230544  0.092221   \n",
              "ENSMUSG00000026124 -0.348092  0.102325  0.652855 -0.099561 -0.117962   \n",
              "ENSMUSG00000026043  0.428279 -0.116995 -0.008734  0.256060  0.100138   \n",
              "ENSMUSG00000045954  0.928849 -0.251801  0.049986 -0.643417 -0.406187   \n",
              "...                      ...       ...       ...       ...       ...   \n",
              "ENSMUSG00000025219 -0.297237 -0.318274 -0.613641  0.154591 -0.229179   \n",
              "ENSMUSG00000025068  0.182135 -0.213085  0.139468  0.225037 -0.179689   \n",
              "ENSMUSG00000064341  0.187736 -0.086645 -0.229711 -0.039657  0.092087   \n",
              "ENSMUSG00000064351  0.116559 -0.030761 -0.230797 -0.029200  0.075384   \n",
              "ENSMUSG00000064358  0.140718  0.019511 -0.206925 -0.021592  0.123875   \n",
              "\n",
              "                          15        16        17        18        19  \n",
              "index                                                                 \n",
              "ENSMUSG00000025902  0.109194  0.531668 -0.102879 -0.657995 -0.244427  \n",
              "ENSMUSG00000025927 -0.049951  0.339507 -0.163870  0.025882 -0.129630  \n",
              "ENSMUSG00000026124  0.027653 -0.273133 -0.023976  0.239798 -0.334595  \n",
              "ENSMUSG00000026043 -0.421274 -0.102574  0.067202  0.233048  0.070419  \n",
              "ENSMUSG00000045954 -0.512795 -0.015923 -1.032671 -0.338712 -0.323017  \n",
              "...                      ...       ...       ...       ...       ...  \n",
              "ENSMUSG00000025219  0.153763 -0.485376  0.330079 -0.250823 -0.429374  \n",
              "ENSMUSG00000025068  0.123369  0.083911  0.152779  0.127375 -0.110744  \n",
              "ENSMUSG00000064341 -0.060499  0.178087  0.184678 -0.028148 -0.145498  \n",
              "ENSMUSG00000064351 -0.033997  0.228394  0.205639 -0.012916 -0.057969  \n",
              "ENSMUSG00000064358 -0.096252  0.196998  0.182893 -0.015379 -0.004771  \n",
              "\n",
              "[1000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d31399cb-c005-49be-beaa-9b34c6a80900\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000025902</th>\n",
              "      <td>-0.114847</td>\n",
              "      <td>0.014870</td>\n",
              "      <td>-0.748335</td>\n",
              "      <td>-0.063984</td>\n",
              "      <td>-0.144368</td>\n",
              "      <td>-0.545247</td>\n",
              "      <td>0.159475</td>\n",
              "      <td>-0.532325</td>\n",
              "      <td>0.221236</td>\n",
              "      <td>-0.063225</td>\n",
              "      <td>-0.833941</td>\n",
              "      <td>0.424912</td>\n",
              "      <td>0.127992</td>\n",
              "      <td>-0.549809</td>\n",
              "      <td>0.059238</td>\n",
              "      <td>0.109194</td>\n",
              "      <td>0.531668</td>\n",
              "      <td>-0.102879</td>\n",
              "      <td>-0.657995</td>\n",
              "      <td>-0.244427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000025927</th>\n",
              "      <td>-0.049106</td>\n",
              "      <td>-0.314564</td>\n",
              "      <td>-0.106663</td>\n",
              "      <td>0.515449</td>\n",
              "      <td>-0.428855</td>\n",
              "      <td>-0.189759</td>\n",
              "      <td>-0.042427</td>\n",
              "      <td>-0.180187</td>\n",
              "      <td>-0.017321</td>\n",
              "      <td>0.513675</td>\n",
              "      <td>0.607979</td>\n",
              "      <td>-0.400233</td>\n",
              "      <td>-0.130607</td>\n",
              "      <td>0.230544</td>\n",
              "      <td>0.092221</td>\n",
              "      <td>-0.049951</td>\n",
              "      <td>0.339507</td>\n",
              "      <td>-0.163870</td>\n",
              "      <td>0.025882</td>\n",
              "      <td>-0.129630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000026124</th>\n",
              "      <td>0.962166</td>\n",
              "      <td>-0.021462</td>\n",
              "      <td>-0.330671</td>\n",
              "      <td>-0.619310</td>\n",
              "      <td>-0.027634</td>\n",
              "      <td>-0.103679</td>\n",
              "      <td>-0.301403</td>\n",
              "      <td>-0.658163</td>\n",
              "      <td>-0.098231</td>\n",
              "      <td>-0.417534</td>\n",
              "      <td>-0.348092</td>\n",
              "      <td>0.102325</td>\n",
              "      <td>0.652855</td>\n",
              "      <td>-0.099561</td>\n",
              "      <td>-0.117962</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>-0.273133</td>\n",
              "      <td>-0.023976</td>\n",
              "      <td>0.239798</td>\n",
              "      <td>-0.334595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000026043</th>\n",
              "      <td>0.456029</td>\n",
              "      <td>0.133721</td>\n",
              "      <td>0.117041</td>\n",
              "      <td>-0.044423</td>\n",
              "      <td>0.051522</td>\n",
              "      <td>-0.259104</td>\n",
              "      <td>0.188649</td>\n",
              "      <td>0.072159</td>\n",
              "      <td>0.050188</td>\n",
              "      <td>0.447276</td>\n",
              "      <td>0.428279</td>\n",
              "      <td>-0.116995</td>\n",
              "      <td>-0.008734</td>\n",
              "      <td>0.256060</td>\n",
              "      <td>0.100138</td>\n",
              "      <td>-0.421274</td>\n",
              "      <td>-0.102574</td>\n",
              "      <td>0.067202</td>\n",
              "      <td>0.233048</td>\n",
              "      <td>0.070419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000045954</th>\n",
              "      <td>0.677873</td>\n",
              "      <td>0.224888</td>\n",
              "      <td>-0.042514</td>\n",
              "      <td>0.125330</td>\n",
              "      <td>-0.433727</td>\n",
              "      <td>-0.706035</td>\n",
              "      <td>0.597891</td>\n",
              "      <td>-0.150792</td>\n",
              "      <td>-0.602343</td>\n",
              "      <td>0.005353</td>\n",
              "      <td>0.928849</td>\n",
              "      <td>-0.251801</td>\n",
              "      <td>0.049986</td>\n",
              "      <td>-0.643417</td>\n",
              "      <td>-0.406187</td>\n",
              "      <td>-0.512795</td>\n",
              "      <td>-0.015923</td>\n",
              "      <td>-1.032671</td>\n",
              "      <td>-0.338712</td>\n",
              "      <td>-0.323017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000025219</th>\n",
              "      <td>0.151430</td>\n",
              "      <td>0.231536</td>\n",
              "      <td>-0.541314</td>\n",
              "      <td>-0.674195</td>\n",
              "      <td>-0.303684</td>\n",
              "      <td>-0.306293</td>\n",
              "      <td>-0.231648</td>\n",
              "      <td>-0.617818</td>\n",
              "      <td>0.060072</td>\n",
              "      <td>0.422939</td>\n",
              "      <td>-0.297237</td>\n",
              "      <td>-0.318274</td>\n",
              "      <td>-0.613641</td>\n",
              "      <td>0.154591</td>\n",
              "      <td>-0.229179</td>\n",
              "      <td>0.153763</td>\n",
              "      <td>-0.485376</td>\n",
              "      <td>0.330079</td>\n",
              "      <td>-0.250823</td>\n",
              "      <td>-0.429374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000025068</th>\n",
              "      <td>-0.373089</td>\n",
              "      <td>0.059569</td>\n",
              "      <td>-0.029015</td>\n",
              "      <td>0.127127</td>\n",
              "      <td>0.058646</td>\n",
              "      <td>0.150118</td>\n",
              "      <td>-0.060132</td>\n",
              "      <td>0.268836</td>\n",
              "      <td>-0.053790</td>\n",
              "      <td>-0.036490</td>\n",
              "      <td>0.182135</td>\n",
              "      <td>-0.213085</td>\n",
              "      <td>0.139468</td>\n",
              "      <td>0.225037</td>\n",
              "      <td>-0.179689</td>\n",
              "      <td>0.123369</td>\n",
              "      <td>0.083911</td>\n",
              "      <td>0.152779</td>\n",
              "      <td>0.127375</td>\n",
              "      <td>-0.110744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000064341</th>\n",
              "      <td>0.062246</td>\n",
              "      <td>0.080869</td>\n",
              "      <td>-0.046041</td>\n",
              "      <td>-0.146250</td>\n",
              "      <td>0.248021</td>\n",
              "      <td>0.042828</td>\n",
              "      <td>-0.493345</td>\n",
              "      <td>0.127642</td>\n",
              "      <td>-0.248297</td>\n",
              "      <td>0.033964</td>\n",
              "      <td>0.187736</td>\n",
              "      <td>-0.086645</td>\n",
              "      <td>-0.229711</td>\n",
              "      <td>-0.039657</td>\n",
              "      <td>0.092087</td>\n",
              "      <td>-0.060499</td>\n",
              "      <td>0.178087</td>\n",
              "      <td>0.184678</td>\n",
              "      <td>-0.028148</td>\n",
              "      <td>-0.145498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000064351</th>\n",
              "      <td>0.043916</td>\n",
              "      <td>0.047881</td>\n",
              "      <td>-0.083317</td>\n",
              "      <td>-0.212095</td>\n",
              "      <td>0.296512</td>\n",
              "      <td>0.086948</td>\n",
              "      <td>-0.551643</td>\n",
              "      <td>0.109832</td>\n",
              "      <td>-0.254271</td>\n",
              "      <td>-0.036927</td>\n",
              "      <td>0.116559</td>\n",
              "      <td>-0.030761</td>\n",
              "      <td>-0.230797</td>\n",
              "      <td>-0.029200</td>\n",
              "      <td>0.075384</td>\n",
              "      <td>-0.033997</td>\n",
              "      <td>0.228394</td>\n",
              "      <td>0.205639</td>\n",
              "      <td>-0.012916</td>\n",
              "      <td>-0.057969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENSMUSG00000064358</th>\n",
              "      <td>0.055735</td>\n",
              "      <td>0.034203</td>\n",
              "      <td>-0.078903</td>\n",
              "      <td>-0.094865</td>\n",
              "      <td>0.303702</td>\n",
              "      <td>0.153246</td>\n",
              "      <td>-0.556047</td>\n",
              "      <td>0.092062</td>\n",
              "      <td>-0.284610</td>\n",
              "      <td>0.048955</td>\n",
              "      <td>0.140718</td>\n",
              "      <td>0.019511</td>\n",
              "      <td>-0.206925</td>\n",
              "      <td>-0.021592</td>\n",
              "      <td>0.123875</td>\n",
              "      <td>-0.096252</td>\n",
              "      <td>0.196998</td>\n",
              "      <td>0.182893</td>\n",
              "      <td>-0.015379</td>\n",
              "      <td>-0.004771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d31399cb-c005-49be-beaa-9b34c6a80900')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d31399cb-c005-49be-beaa-9b34c6a80900 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d31399cb-c005-49be-beaa-9b34c6a80900');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jncrET3D5bL3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}